{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                           speaker sex  \\\n",
      "0  tr18146  ca2031caa4032c51980160359953d507   M   \n",
      "1  tr18147  4cee0addb3c69f6866869b180f90d45f   M   \n",
      "2  tr18148  b3d7f76d74ec268492f8190ca123a6b2   M   \n",
      "3  tr18149  722efac7138c8197a9d1e97eed3a8b18   M   \n",
      "4  tr18150  fcc61122f3553c57ae207adeb1a1af84   M   \n",
      "\n",
      "                                                text  \\\n",
      "0  Yeni yasama dÃ¶neminin Ã¼lkemiz iÃ§in, milletimiz...   \n",
      "1  SayÄ±n BaÅŸkan, deÄŸerli milletvekilleri; bugÃ¼n, ...   \n",
      "2  SayÄ±n BaÅŸkanÄ±m, Ã¶ncelikle yÃ¼ce Meclisin BaÅŸkan...   \n",
      "3  24â€™Ã¼ncÃ¼ DÃ¶nem Meclis BaÅŸkanlÄ±ÄŸÄ±na seÃ§ilmenizde...   \n",
      "4  Usul tartÄ±ÅŸmasÄ±nda 2 kiÅŸi lehte 2 kiÅŸi aleyhte...   \n",
      "\n",
      "                                             text_en  label  \n",
      "0  Mr. President, dear lawmakers, I salute you, a...      0  \n",
      "1  Mr. President, members of lawmakers, as I spea...      0  \n",
      "2  Mr. President, I'm here to share with you the ...      0  \n",
      "3  Mr. President, under the principles determined...      0  \n",
      "4  Two in favour of two in the legal debate, Mr. ...      1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17384 entries, 0 to 17383\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       17384 non-null  object\n",
      " 1   speaker  17384 non-null  object\n",
      " 2   sex      17384 non-null  object\n",
      " 3   text     17384 non-null  object\n",
      " 4   text_en  17384 non-null  object\n",
      " 5   label    17384 non-null  int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 815.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "p_dataset_path = \"./power/power-tr-train.tsv\"\n",
    "p_data = pd.read_csv(p_dataset_path, sep=\"\\t\")\n",
    "\n",
    "# Display basic information\n",
    "print(p_data.head())\n",
    "print(p_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    8932\n",
      "0    8452\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing translations or text fields\n",
    "p_data = p_data.dropna(subset=['text', 'label'])\n",
    "\n",
    "# Display class distribution\n",
    "print(p_data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 15645, Test size: 1739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "p_train_data, p_test_data = train_test_split(\n",
    "    p_data, test_size=0.1, stratify=p_data['label'], random_state=42\n",
    ")\n",
    "print(f\"Training size: {len(p_train_data)}, Test size: {len(p_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15645/15645 [00:13<00:00, 1144.76 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1739/1739 [00:01<00:00, 1264.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "# Convert pandas DataFrame to Hugging Face Dataset\n",
    "p_train_dataset = Dataset.from_pandas(p_train_data)\n",
    "p_test_dataset = Dataset.from_pandas(p_test_data)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train_p = p_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_p = p_test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "tokenized_train_p = tokenized_train_p.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "tokenized_test_p = tokenized_test_p.remove_columns([\"text\", \"__index_level_0__\"])\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_train_p.set_format(\"torch\")\n",
    "tokenized_test_p.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'speaker', 'sex', 'text', 'label', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 15645\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\dor_b\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\dor_b\\AppData\\Local\\Temp\\ipykernel_15780\\951444828.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  2%|â–         | 50/2934 [05:24<5:34:35,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6702, 'grad_norm': 5.592130184173584, 'learning_rate': 2.948875255623722e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 100/2934 [11:12<5:32:40,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.554, 'grad_norm': 10.038786888122559, 'learning_rate': 2.8977505112474437e-05, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 150/2934 [17:03<5:31:28,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4963, 'grad_norm': 7.311159610748291, 'learning_rate': 2.8466257668711656e-05, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 200/2934 [23:04<5:23:12,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5499, 'grad_norm': 8.739221572875977, 'learning_rate': 2.7955010224948877e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–Š         | 250/2934 [28:53<5:06:57,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4433, 'grad_norm': 15.393159866333008, 'learning_rate': 2.7443762781186092e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 300/2934 [34:38<5:02:30,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.405, 'grad_norm': 16.861812591552734, 'learning_rate': 2.6932515337423314e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 350/2934 [40:23<4:57:41,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4208, 'grad_norm': 6.214046955108643, 'learning_rate': 2.6421267893660532e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–Ž        | 400/2934 [46:08<4:48:23,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4149, 'grad_norm': 5.763175964355469, 'learning_rate': 2.591002044989775e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 450/2934 [52:03<4:53:20,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4263, 'grad_norm': 10.247779846191406, 'learning_rate': 2.539877300613497e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 500/2934 [57:58<4:41:35,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3687, 'grad_norm': 12.904627799987793, 'learning_rate': 2.488752556237219e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–Š        | 550/2934 [1:03:51<4:43:44,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4202, 'grad_norm': 11.398418426513672, 'learning_rate': 2.4376278118609406e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 600/2934 [1:09:43<4:35:02,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4123, 'grad_norm': 6.457627773284912, 'learning_rate': 2.3865030674846628e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 650/2934 [1:15:31<4:25:28,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4003, 'grad_norm': 7.628976821899414, 'learning_rate': 2.3353783231083846e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 700/2934 [1:21:18<4:16:02,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3812, 'grad_norm': 5.1510910987854, 'learning_rate': 2.2842535787321064e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 750/2934 [1:27:07<4:08:55,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3425, 'grad_norm': 10.644410133361816, 'learning_rate': 2.2331288343558283e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 800/2934 [1:32:53<4:08:20,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3661, 'grad_norm': 12.402985572814941, 'learning_rate': 2.18200408997955e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–‰       | 850/2934 [1:38:39<3:54:52,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3633, 'grad_norm': 10.18461799621582, 'learning_rate': 2.130879345603272e-05, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 900/2934 [1:44:26<3:55:37,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3647, 'grad_norm': 6.4427995681762695, 'learning_rate': 2.0797546012269938e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 950/2934 [1:50:12<3:50:19,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.304, 'grad_norm': 19.184078216552734, 'learning_rate': 2.028629856850716e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 978/2934 [1:53:20<3:10:21,  5.84s/it]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 978/2934 [1:54:39<3:10:21,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.367783784866333, 'eval_runtime': 78.612, 'eval_samples_per_second': 22.121, 'eval_steps_per_second': 1.387, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 1000/2934 [1:57:37<3:45:01,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3586, 'grad_norm': 12.657052993774414, 'learning_rate': 1.9775051124744374e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1050/2934 [2:03:26<3:41:07,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2921, 'grad_norm': 11.38012409210205, 'learning_rate': 1.9263803680981596e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 1100/2934 [2:09:11<3:30:31,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2403, 'grad_norm': 4.62935209274292, 'learning_rate': 1.8752556237218814e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 1150/2934 [2:14:57<3:24:54,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2907, 'grad_norm': 16.685230255126953, 'learning_rate': 1.8241308793456033e-05, 'epoch': 1.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 1200/2934 [2:20:44<3:22:14,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2504, 'grad_norm': 11.056654930114746, 'learning_rate': 1.773006134969325e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 1250/2934 [2:26:34<3:16:47,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2545, 'grad_norm': 11.11767864227295, 'learning_rate': 1.7218813905930473e-05, 'epoch': 1.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1300/2934 [2:32:23<3:09:24,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2691, 'grad_norm': 4.227703094482422, 'learning_rate': 1.6707566462167688e-05, 'epoch': 1.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1350/2934 [2:38:11<3:06:37,  7.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2732, 'grad_norm': 12.931123733520508, 'learning_rate': 1.619631901840491e-05, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1400/2934 [2:43:57<2:55:46,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2526, 'grad_norm': 8.294759750366211, 'learning_rate': 1.5685071574642128e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1450/2934 [2:49:45<2:54:59,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2863, 'grad_norm': 8.18204116821289, 'learning_rate': 1.5173824130879344e-05, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1500/2934 [2:55:33<2:46:21,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2747, 'grad_norm': 7.145063400268555, 'learning_rate': 1.4662576687116564e-05, 'epoch': 1.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 1550/2934 [3:01:20<2:37:20,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2741, 'grad_norm': 14.762751579284668, 'learning_rate': 1.4151329243353784e-05, 'epoch': 1.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1600/2934 [3:07:07<2:34:07,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2886, 'grad_norm': 1.9343256950378418, 'learning_rate': 1.3640081799591003e-05, 'epoch': 1.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1650/2934 [3:12:53<2:26:00,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2468, 'grad_norm': 9.682028770446777, 'learning_rate': 1.3128834355828221e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1700/2934 [3:18:37<2:19:08,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2414, 'grad_norm': 8.62824821472168, 'learning_rate': 1.2617586912065441e-05, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1750/2934 [3:24:22<2:16:31,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.274, 'grad_norm': 16.790782928466797, 'learning_rate': 1.210633946830266e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1800/2934 [3:30:06<2:09:52,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2929, 'grad_norm': 26.221187591552734, 'learning_rate': 1.1595092024539878e-05, 'epoch': 1.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 1850/2934 [3:35:50<2:05:12,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3066, 'grad_norm': 8.270427703857422, 'learning_rate': 1.1083844580777098e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1900/2934 [3:41:32<1:57:30,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2118, 'grad_norm': 3.5100669860839844, 'learning_rate': 1.0572597137014316e-05, 'epoch': 1.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1950/2934 [3:47:17<1:51:48,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2368, 'grad_norm': 14.030057907104492, 'learning_rate': 1.0061349693251534e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1956/2934 [3:47:53<1:32:43,  5.69s/it]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1956/2934 [3:49:08<1:32:43,  5.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3140605092048645, 'eval_runtime': 75.2491, 'eval_samples_per_second': 23.11, 'eval_steps_per_second': 1.449, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 2000/2934 [3:54:25<1:46:16,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.204, 'grad_norm': 8.990534782409668, 'learning_rate': 9.550102249488754e-06, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 2050/2934 [4:00:08<1:40:59,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1557, 'grad_norm': 12.580320358276367, 'learning_rate': 9.038854805725971e-06, 'epoch': 2.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2100/2934 [4:05:51<1:34:19,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1844, 'grad_norm': 12.073935508728027, 'learning_rate': 8.52760736196319e-06, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 2150/2934 [4:11:36<1:29:58,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1618, 'grad_norm': 8.442102432250977, 'learning_rate': 8.01635991820041e-06, 'epoch': 2.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2200/2934 [4:17:12<1:23:13,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.18, 'grad_norm': 23.259061813354492, 'learning_rate': 7.5051124744376285e-06, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 2250/2934 [4:22:51<1:17:48,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1854, 'grad_norm': 3.674595832824707, 'learning_rate': 6.993865030674846e-06, 'epoch': 2.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2300/2934 [4:28:29<1:10:51,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2, 'grad_norm': 22.701766967773438, 'learning_rate': 6.482617586912065e-06, 'epoch': 2.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 2350/2934 [4:34:12<1:07:09,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1771, 'grad_norm': 1.8274117708206177, 'learning_rate': 5.971370143149284e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2400/2934 [4:39:59<1:01:30,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2027, 'grad_norm': 3.2407474517822266, 'learning_rate': 5.4601226993865036e-06, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 2450/2934 [4:45:46<56:36,  7.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1382, 'grad_norm': 30.341142654418945, 'learning_rate': 4.948875255623722e-06, 'epoch': 2.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2500/2934 [4:51:41<50:57,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1616, 'grad_norm': 12.912763595581055, 'learning_rate': 4.437627811860941e-06, 'epoch': 2.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 2550/2934 [4:57:32<46:17,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.162, 'grad_norm': 0.6402721405029297, 'learning_rate': 3.92638036809816e-06, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 2600/2934 [5:03:25<39:43,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.164, 'grad_norm': 4.863226413726807, 'learning_rate': 3.415132924335378e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 2650/2934 [5:09:22<33:31,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2015, 'grad_norm': 9.845714569091797, 'learning_rate': 2.9038854805725973e-06, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2700/2934 [5:15:18<28:25,  7.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2026, 'grad_norm': 2.560894250869751, 'learning_rate': 2.392638036809816e-06, 'epoch': 2.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 2750/2934 [5:21:14<22:04,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1713, 'grad_norm': 16.745559692382812, 'learning_rate': 1.881390593047035e-06, 'epoch': 2.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 2800/2934 [5:27:13<15:52,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1991, 'grad_norm': 41.98064422607422, 'learning_rate': 1.3701431492842536e-06, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 2850/2934 [5:33:08<09:59,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2, 'grad_norm': 18.784271240234375, 'learning_rate': 8.588957055214724e-07, 'epoch': 2.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 2900/2934 [5:39:01<03:58,  7.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.169, 'grad_norm': 4.15236234664917, 'learning_rate': 3.4764826175869123e-07, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2934/2934 [5:42:57<00:00,  6.02s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2934/2934 [5:44:19<00:00,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3779737651348114, 'eval_runtime': 64.5917, 'eval_samples_per_second': 26.923, 'eval_steps_per_second': 1.688, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2934/2934 [5:44:34<00:00,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 20674.1867, 'train_samples_per_second': 2.27, 'train_steps_per_second': 0.142, 'train_loss': 0.29087191157870823, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2934, training_loss=0.29087191157870823, metrics={'train_runtime': 20674.1867, 'train_samples_per_second': 2.27, 'train_steps_per_second': 0.142, 'total_flos': 1.23491173833216e+16, 'train_loss': 0.29087191157870823, 'epoch': 3.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "output_dir = 'C:\\\\Users\\\\dor_b\\\\Documents\\\\results'\n",
    "log = 'C:\\\\Users\\\\dor_b\\\\Documents\\\\logs'\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=log,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_p,\n",
    "    eval_dataset=tokenized_test_p,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./xlm_roberta_model_power_tr\\\\tokenizer_config.json',\n",
       " './xlm_roberta_model_power_tr\\\\special_tokens_map.json',\n",
       " './xlm_roberta_model_power_tr\\\\tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./xlm_roberta_model_power_tr\")\n",
    "tokenizer.save_pretrained(\"./xlm_roberta_model_power_tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import huggingface_hub\n",
    "\n",
    "\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    "    use_auth_token=\"your token here\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "def classification_pipeline(speech):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"instruction\", \"content\": \"\"\"Assume you are politician, which will be asked to classify the ideology of the\n",
    "                                                speakers' party from turkish parliament. Speech will be given as a text. \n",
    "                                                In other words, this involves performing binary classification to determine whether\n",
    "                                                the speakerâ€™s party is governing (0) or oppositon (1). You are not allowed to answer \n",
    "                                                anything else besides the number 1 or 0.\"\"\"},\n",
    "        {\"role\": \"input\", \"content\": speech},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.01,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    return outputs[0][\"generated_text\"][len(prompt):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tokenized_test_p.iterrows():\n",
    "    print(classification_pipeline(row['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tokenized_test_p.iterrows():\n",
    "    print(classification_pipeline(row['text_en']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
