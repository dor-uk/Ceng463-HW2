{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id                           speaker sex  \\\n",
      "0  tr18146  ca2031caa4032c51980160359953d507   M   \n",
      "1  tr18147  4cee0addb3c69f6866869b180f90d45f   M   \n",
      "2  tr18148  b3d7f76d74ec268492f8190ca123a6b2   M   \n",
      "3  tr18149  722efac7138c8197a9d1e97eed3a8b18   M   \n",
      "4  tr18150  fcc61122f3553c57ae207adeb1a1af84   M   \n",
      "\n",
      "                                                text  \\\n",
      "0  Yeni yasama döneminin ülkemiz için, milletimiz...   \n",
      "1  Sayın Başkan, değerli milletvekilleri; bugün, ...   \n",
      "2  Sayın Başkanım, öncelikle yüce Meclisin Başkan...   \n",
      "3  24’üncü Dönem Meclis Başkanlığına seçilmenizde...   \n",
      "4  Usul tartışmasında 2 kişi lehte 2 kişi aleyhte...   \n",
      "\n",
      "                                             text_en  label  \n",
      "0  Mr. President, dear lawmakers, I salute you, a...      0  \n",
      "1  Mr. President, members of lawmakers, as I spea...      0  \n",
      "2  Mr. President, I'm here to share with you the ...      0  \n",
      "3  Mr. President, under the principles determined...      0  \n",
      "4  Two in favour of two in the legal debate, Mr. ...      1  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17384 entries, 0 to 17383\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       17384 non-null  object\n",
      " 1   speaker  17384 non-null  object\n",
      " 2   sex      17384 non-null  object\n",
      " 3   text     17384 non-null  object\n",
      " 4   text_en  17384 non-null  object\n",
      " 5   label    17384 non-null  int64 \n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 815.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "p_dataset_path = \"./power/power-tr-train.tsv\"\n",
    "p_data = pd.read_csv(p_dataset_path, sep=\"\\t\")\n",
    "\n",
    "# Display basic information\n",
    "print(p_data.head())\n",
    "print(p_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    8932\n",
      "0    8452\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing translations or text fields\n",
    "p_data = p_data.dropna(subset=['text', 'label'])\n",
    "\n",
    "# Display class distribution\n",
    "print(p_data['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 15645, Test size: 1739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "p_train_data, p_test_data = train_test_split(\n",
    "    p_data, test_size=0.1, stratify=p_data['label'], random_state=42\n",
    ")\n",
    "print(f\"Training size: {len(p_train_data)}, Test size: {len(p_test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the saved model directory\n",
    "model_dir = \"./xlm_roberta_model_power_tr\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1739/1739 [00:01<00:00, 1191.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert to Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(p_test_data)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dor_b\\AppData\\Local\\Temp\\ipykernel_20592\\1229279618.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16,  # Adjust batch size as per your hardware\n",
    "    logging_dir=\"./logs\",\n",
    "    do_train=False,\n",
    "    do_eval=True\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [00:32<00:00,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.74      0.81       845\n",
      "           1       0.79      0.91      0.84       894\n",
      "\n",
      "    accuracy                           0.83      1739\n",
      "   macro avg       0.84      0.82      0.83      1739\n",
      "weighted avg       0.84      0.83      0.83      1739\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# Get predictions\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# Extract predicted labels\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# True labels\n",
    "true_labels = p_test_data[\"label\"].values\n",
    "\n",
    "# Print Classification Report\n",
    "print(classification_report(true_labels, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
